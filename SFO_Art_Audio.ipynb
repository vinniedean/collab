{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjSkBePSt4lM+SOANMRv/c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinniedean/collab/blob/main/SFO_Art_Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n"
      ],
      "metadata": {
        "id": "IloETAFUcEil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Load the model, for example, the \"small\" model\n",
        "model = whisper.load_model(\"small\")\n"
      ],
      "metadata": {
        "id": "1su-kGEHcLOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your audio file\n",
        "audio_path = \"/content/International_Main_Hall_4.m4a\"\n",
        "\n",
        "# Transcribe the audio\n",
        "result = model.transcribe(audio_path)\n",
        "\n",
        "# Print the transcription\n",
        "print(result[\"text\"])\n"
      ],
      "metadata": {
        "id": "hrTbuFoLcRMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf nltk gensim matplotlib seaborn\n"
      ],
      "metadata": {
        "id": "Ck90KAyIciLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "PMVX-Jx_c-7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "WA21BeTAdCrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = '/content/Rupert Garcia.pdf'  # Update this path\n",
        "doc = fitz.open(pdf_path)\n",
        "text = \"\"\n",
        "for page in doc:\n",
        "    text += page.get_text()\n"
      ],
      "metadata": {
        "id": "p_hButyrdMKB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "cleaned_text = preprocess_text(text)\n"
      ],
      "metadata": {
        "id": "qx7fcAf2dS4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "# Tokenize cleaned text\n",
        "tokens = [word for word in cleaned_text.split()]\n",
        "\n",
        "# Create a dictionary and corpus for topic modeling\n",
        "dictionary = corpora.Dictionary([tokens])\n",
        "corpus = [dictionary.doc2bow(token) for token in [tokens]]\n",
        "\n",
        "# Using LDA for topic modeling\n",
        "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print topics\n",
        "topics = ldamodel.print_topics(num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "id": "K2IAkWaxdXoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "word_counts = Counter(tokens)\n",
        "most_common_words = word_counts.most_common(10)\n",
        "\n",
        "words = [word[0] for word in most_common_words]\n",
        "counts = [word[1] for word in most_common_words]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=words, y=counts)\n",
        "plt.title('Most Common Words')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7DCgYqJLdaQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx matplotlib\n"
      ],
      "metadata": {
        "id": "E_ytTSaZdg9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "JWFPQngqdj3p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Assuming `cleaned_text` is your preprocessed text data\n",
        "sentences = sent_tokenize(cleaned_text)\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n"
      ],
      "metadata": {
        "id": "MojTcIh0dmkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the window size (sentence-wise co-occurrence)\n",
        "window_size = 2  # This is just illustrative; we're using sentences, so it's not used here\n",
        "\n",
        "# Create a list of all word pairs within the same sentence\n",
        "word_pairs = []\n",
        "for sentence in tokenized_sentences:\n",
        "    # Using combinations to create pairs of words in the same sentence\n",
        "    word_pairs.extend(list(combinations(sentence, 2)))\n",
        "\n",
        "# Count the occurrences of each pair to understand the strength of connections\n",
        "pair_counts = Counter(word_pairs)\n"
      ],
      "metadata": {
        "id": "jpnvuW_8dpcQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges between words with weights\n",
        "for pair, count in pair_counts.items():\n",
        "    word1, word2 = pair\n",
        "    G.add_edge(word1, word2, weight=count)\n"
      ],
      "metadata": {
        "id": "ihsmku2fdsdO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "# Layout for our nodes\n",
        "pos = nx.spring_layout(G, k=0.1)\n",
        "\n",
        "# Drawing the graph\n",
        "nx.draw(G, pos, with_labels=True, node_color='skyblue', edge_color='k', linewidths=1, font_size=20, node_size=10, alpha=0.7)\n",
        "\n",
        "# Considering edge weights\n",
        "edge_weights = nx.get_edge_attributes(G, 'weight')\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_weights)\n",
        "\n",
        "plt.title(\"Word Co-occurrence Network\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m5JFW0fHdvEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate degree centrality\n",
        "degrees = dict(G.degree())\n",
        "\n",
        "# Sort nodes by degree centrality\n",
        "sorted_degrees = sorted(degrees.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Let's say we're interested in the top 5 nodes\n",
        "key_nodes = [node[0] for node in sorted_degrees[:5]]\n",
        "print(\"Key nodes:\", key_nodes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfogTKz1fDlu",
        "outputId": "a00d3796-ec3b-40cc-b933-1b16703a5991"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key nodes: ['bird', 'technology', 'work', 'symbolizes', 'natural']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a key node. For demonstration, let's use the first key node identified\n",
        "key_node = key_nodes[0]\n",
        "\n",
        "# Extract a subgraph centered around the key node\n",
        "# This example uses ego_graph to get all nodes connected directly to the key node\n",
        "subgraph = nx.ego_graph(G, key_node)\n",
        "\n",
        "# For a larger radius (more steps away), you can use the radius parameter\n",
        "# subgraph = nx.ego_graph(G, key_node, radius=2)\n"
      ],
      "metadata": {
        "id": "qLforXLUfHM2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "pos = nx.spring_layout(subgraph, k=0.5)\n",
        "nx.draw(subgraph, pos, with_labels=True, node_color='lightblue', edge_color='gray', linewidths=1, font_size=12, node_size=500, alpha=0.7)\n",
        "plt.title(f\"Subgraph Centered Around Node: {key_node}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xl_5uOccfKq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}